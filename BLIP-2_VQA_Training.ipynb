{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0576141",
   "metadata": {},
   "source": [
    "Cell 1: Import Libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9814bd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations/v2_OpenEnded_mscoco_train2014_questions.json -> True\n",
      "annotations/v2_mscoco_train2014_annotations.json -> True\n",
      "annotations/v2_OpenEnded_mscoco_val2014_questions.json -> True\n",
      "annotations/v2_mscoco_val2014_annotations.json -> True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "BASE_PATH = \"annotations\"\n",
    "\n",
    "TRAIN_Q_PATH = f\"{BASE_PATH}/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "TRAIN_A_PATH = f\"{BASE_PATH}/v2_mscoco_train2014_annotations.json\"\n",
    "\n",
    "VAL_Q_PATH   = f\"{BASE_PATH}/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "VAL_A_PATH   = f\"{BASE_PATH}/v2_mscoco_val2014_annotations.json\"\n",
    "for p in [TRAIN_Q_PATH, TRAIN_A_PATH, VAL_Q_PATH, VAL_A_PATH]:\n",
    "    print(p, \"->\", os.path.exists(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d6749",
   "metadata": {},
   "source": [
    "Cell 2: Load VQA dataset loader and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70be9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vqa(q_path, a_path):\n",
    "    with open(q_path, 'r') as f:\n",
    "        questions = json.load(f)[\"questions\"]\n",
    "    with open(a_path, 'r') as f:\n",
    "        annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "    ann_dict = {ann[\"question_id\"]: ann for ann in annotations}\n",
    "\n",
    "    data = []\n",
    "    for q in questions:\n",
    "        qid = q[\"question_id\"]\n",
    "        if qid in ann_dict:\n",
    "            data.append({\n",
    "                \"image_id\": q[\"image_id\"],\n",
    "                \"question\": q[\"question\"].lower().strip(),\n",
    "                \"answers\": ann_dict[qid][\"answers\"]\n",
    "            })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5149801",
   "metadata": {},
   "source": [
    "Cell 3: Seperating data by question type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88766a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_type(question):\n",
    "    q = question.lower().strip()\n",
    "    \n",
    "    # Yes/no questions\n",
    "    if q.startswith((\"is there\", \"are there\", \"does\", \"do\", \"can\", \"could\")):\n",
    "        return \"yes/no\"\n",
    "    elif q.startswith((\"is \", \"are \", \"was \", \"were \")):\n",
    "        words = q.split()\n",
    "        if len(words) > 2: \n",
    "            return \"yes/no\"\n",
    "    \n",
    "    # Other types\n",
    "    if q.startswith(\"how many\"):\n",
    "        return \"how many\"\n",
    "    elif q.startswith(\"what\"):\n",
    "        return \"what\"\n",
    "    elif q.startswith(\"where\"):\n",
    "        return \"where\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "def group_by_type(data):\n",
    "    buckets = defaultdict(list)\n",
    "    for item in data:\n",
    "        q_type = get_question_type(item[\"question\"])\n",
    "        buckets[q_type].append(item)\n",
    "    return buckets\n",
    "\n",
    "\n",
    "def balanced_sample(buckets, total_samples, ratios):\n",
    "    sampled = []\n",
    "\n",
    "    for q_type, ratio in ratios.items():\n",
    "        target = int(total_samples * ratio)\n",
    "        available = buckets.get(q_type, [])\n",
    "\n",
    "        if len(available) < target:\n",
    "            sampled.extend(available)\n",
    "        else:\n",
    "            sampled.extend(random.sample(available, target))\n",
    "\n",
    "    random.shuffle(sampled)\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d751fcd",
   "metadata": {},
   "source": [
    "Cell 4: Calculating training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252d9dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training set size: 5000\n"
     ]
    }
   ],
   "source": [
    "train_data = load_vqa(TRAIN_Q_PATH, TRAIN_A_PATH)\n",
    "val_data   = load_vqa(VAL_Q_PATH, VAL_A_PATH)\n",
    "\n",
    "train_buckets = group_by_type(train_data)\n",
    "\n",
    "TRAIN_SAMPLES = 5000  \n",
    "\n",
    "ratios = {\n",
    "    \"what\": 0.35,\n",
    "    \"yes/no\": 0.25,\n",
    "    \"how many\": 0.20,\n",
    "    \"other\": 0.20\n",
    "}\n",
    "\n",
    "balanced_train = balanced_sample(train_buckets, TRAIN_SAMPLES, ratios)\n",
    "\n",
    "print(\"Balanced training set size:\", len(balanced_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a461cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 3000\n",
      "Test: 2000\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(val_data)\n",
    "\n",
    "val_set  = val_data[:3000]\n",
    "test_set = val_data[3000:5000]\n",
    "\n",
    "print(\"Validation:\", len(val_set))\n",
    "print(\"Test:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99291141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 4788\n",
      "Validation images: 2820\n",
      "Test images: 1926\n"
     ]
    }
   ],
   "source": [
    "balanced_train   \n",
    "val_set        \n",
    "test_set        \n",
    "\n",
    "train_images = len(set([item['image_id'] for item in balanced_train]))\n",
    "val_images   = len(set([item['image_id'] for item in val_set]))\n",
    "test_images  = len(set([item['image_id'] for item in test_set]))\n",
    "\n",
    "print(f\"Training images: {train_images}\")\n",
    "print(f\"Validation images: {val_images}\")\n",
    "print(f\"Test images: {test_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b28ec04",
   "metadata": {},
   "source": [
    "Cell 5: Question Type Distribution Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e80c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distribution(data, name):\n",
    "    counts = {\"what\": 0, \"yes/no\": 0, \"how many\": 0, \"other\": 0}\n",
    "    for item in data:\n",
    "        counts[get_question_type(item[\"question\"])] += 1\n",
    "    \n",
    "    print(f\"\\n{name} Distribution:\")\n",
    "    total = sum(counts.values())\n",
    "    for q_type, count in counts.items():\n",
    "        print(f\"  {q_type}: {count} ({count/total*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7af59",
   "metadata": {},
   "source": [
    "Cell 6: Helper to extract majorit answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7472fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_majority_answer(answers):\n",
    "    answer_list = [a[\"answer\"].lower().strip() for a in answers]\n",
    "    return Counter(answer_list).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0924f6d",
   "metadata": {},
   "source": [
    "Cell 7: Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87a6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, data, image_dir, processor, split=\"train\"):\n",
    "        self.data = data\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.split = split  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        image_id = item[\"image_id\"]\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            filename = f\"COCO_train2014_{image_id:012d}.jpg\"\n",
    "        else: \n",
    "            filename = f\"COCO_val2014_{image_id:012d}.jpg\"\n",
    "        \n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            alt_filenames = [\n",
    "                f\"COCO_train2014_{image_id:012d}.jpg\",\n",
    "                f\"COCO_val2014_{image_id:012d}.jpg\",\n",
    "                f\"{image_id:012d}.jpg\"\n",
    "            ]\n",
    "            \n",
    "            for alt_filename in alt_filenames:\n",
    "                alt_path = os.path.join(self.image_dir, alt_filename)\n",
    "                if os.path.exists(alt_path):\n",
    "                    image = Image.open(alt_path).convert(\"RGB\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Warning: Image not found: {filename}\")\n",
    "                image = Image.new('RGB', (224, 224), color='white')\n",
    "\n",
    "        question = item[\"question\"]\n",
    "        answer = get_majority_answer(item[\"answers\"])\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=question,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=32\n",
    "        )\n",
    "        \n",
    "        answer_encoding = self.processor.tokenizer(\n",
    "            answer,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=32\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = answer_encoding[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1158da",
   "metadata": {},
   "source": [
    "Cell 8: Loading BLIP_2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36df86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\.RAKSHYA\\Neural Network\\VQAv2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "The image processor of type `BlipImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "Loading weights: 100%|██████████| 1247/1247 [00:01<00:00, 637.54it/s, Materializing param=vision_model.post_layernorm.weight]                               \n"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2ForConditionalGeneration, Blip2Processor\n",
    "import torch\n",
    "\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc765c0f",
   "metadata": {},
   "source": [
    " Cell 9: VQA data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a1867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 5000 samples\n",
      "Val: 3000 samples\n",
      "Test: 2000 samples\n",
      "\n",
      "DataLoader sizes:\n",
      "Train batches: 625\n",
      "Val batches: 375\n",
      "Test batches: 250\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    max_input_length = max(item[\"input_ids\"].shape[0] for item in batch)\n",
    "    max_label_length = max(item[\"labels\"].shape[0] for item in batch)\n",
    "    \n",
    "    pixel_values = []\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in batch:\n",
    "        pixel_values.append(item[\"pixel_values\"])\n",
    "        \n",
    "        pad_len = max_input_length - item[\"input_ids\"].shape[0]\n",
    "        if pad_len > 0:\n",
    "            padded_input = F.pad(\n",
    "                item[\"input_ids\"],\n",
    "                (0, pad_len),\n",
    "                value=processor.tokenizer.pad_token_id\n",
    "            )\n",
    "            padded_attention = F.pad(\n",
    "                item[\"attention_mask\"],\n",
    "                (0, pad_len),\n",
    "                value=0\n",
    "            )\n",
    "        else:\n",
    "            padded_input = item[\"input_ids\"]\n",
    "            padded_attention = item[\"attention_mask\"]\n",
    "        \n",
    "        input_ids.append(padded_input)\n",
    "        attention_mask.append(padded_attention)\n",
    "        \n",
    "        pad_len_labels = max_label_length - item[\"labels\"].shape[0]\n",
    "        if pad_len_labels > 0:\n",
    "            padded_labels = F.pad(\n",
    "                item[\"labels\"],\n",
    "                (0, pad_len_labels),\n",
    "                value=-100\n",
    "            )\n",
    "        else:\n",
    "            padded_labels = item[\"labels\"]\n",
    "        \n",
    "        labels.append(padded_labels)\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": torch.stack(pixel_values),\n",
    "        \"input_ids\": torch.stack(input_ids),\n",
    "        \"attention_mask\": torch.stack(attention_mask),\n",
    "        \"labels\": torch.stack(labels)\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VQADataset(\n",
    "    data=balanced_train,\n",
    "    image_dir=\"train2014\", \n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "val_dataset = VQADataset(\n",
    "    data=val_set,\n",
    "    image_dir=\"val2014\", \n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "test_dataset = VQADataset(\n",
    "    data=test_set,\n",
    "    image_dir=\"val2014\", \n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader sizes:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501698ba",
   "metadata": {},
   "source": [
    "Cell 10: Fine tuning BLIP-2 Q-Former "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "500efdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 105,137,664\n",
      "Total parameters: 3,744,761,856\n",
      "Percentage trainable: 2.81%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Blip2ForConditionalGeneration(\n",
       "  (vision_model): Blip2VisionModel(\n",
       "    (embeddings): Blip2VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): Blip2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-38): 39 x Blip2EncoderLayer(\n",
       "          (self_attn): Blip2Attention(\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Blip2MLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (qformer): Blip2QFormerModel(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (encoder): Blip2QFormerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Linear(in_features=768, out_features=2560, bias=True)\n",
       "  (language_model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50304, 2560, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Freeze other, only train Q-Former\n",
    "for name, param in model.named_parameters():\n",
    "    if \"qformer\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Percentage trainable: {trainable_params/total_params*100:.2f}%\")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-5,  \n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2774a",
   "metadata": {},
   "source": [
    "Cell 11: Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2036cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"],\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"loss\": loss.item(),\n",
    "            \"lr\": scheduler.get_last_lr()[0]\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validation\")\n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e669e39",
   "metadata": {},
   "source": [
    "Cell 12: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b3d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [02:02<00:00,  5.11it/s, loss=0.62, lr=5e-05]\n",
      "Validation: 100%|██████████| 375/375 [00:49<00:00,  7.55it/s, loss=0.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      " Training Loss:   0.623415 (Time: 122.38s)\n",
      " Validation Loss: 0.521839 (Time: 49.69s)\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [02:01<00:00,  5.13it/s, loss=0.54, lr=3e-05]\n",
      "Validation: 100%|██████████| 375/375 [00:48<00:00,  7.72it/s, loss=0.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      " Training Loss:   0.543789 (Time: 121.77s)\n",
      " Validation Loss: 0.462415 (Time: 48.60s)\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 625/625 [02:00<00:00,  5.19it/s, loss=0.50, lr=1e-05]\n",
      "Validation: 100%|██████████| 375/375 [00:47<00:00,  7.85it/s, loss=0.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      " Training Loss:   0.500058 (Time: 120.35s)\n",
      " Validation Loss: 0.426926 (Time: 47.78s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Validate\n",
    "    start_time = time.time()\n",
    "    val_loss = validate_epoch(model, val_loader, device)\n",
    "    val_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Training Loss: {train_loss:.6f} (Time: {train_time:.2f}s)\")\n",
    "    print(f\"  Validation Loss:   {val_loss:.6f} (Time: {val_time:.2f}s)\")\n",
    "    print ()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        # Save checkpoint \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(), \n",
    "            'optimizer_state_dict': optimizer.state_dict(), \n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_epoch': best_epoch\n",
    "        }, \"final_checkpoint.pt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad058b4",
   "metadata": {},
   "source": [
    "Cell 13: Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a51e766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 3\n",
      "Train loss: 0.5000576014062645\n",
      "Val loss: 0.4269264387957593\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"final_checkpoint.pt\", map_location=\"cpu\")\n",
    "\n",
    "print(\"Best epoch:\", ckpt[\"epoch\"])\n",
    "print(\"Train loss:\", ckpt[\"train_loss\"])\n",
    "print(\"Val loss:\", ckpt[\"val_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
